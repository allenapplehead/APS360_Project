{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ð„†  Melogen Training Script  ð„‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from USB\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pretty_midi\n",
    "from model import Net\n",
    "from midi_to_piano_roll import midi_to_piano_roll\n",
    "from loss import blur_loss\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGS ###\n",
    "\n",
    "# Test code on small dataset, half dataset, or on full dataset\n",
    "MODE = \"full\"\n",
    "\n",
    "# Set the batch size\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Set loader style\n",
    "LOADER = \"single\"\n",
    "LOAD_BATCH_SIZE = 250\n",
    "\n",
    "# If we already saved the npy's no need to resave\n",
    "NOT_YET_LOADED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"full\":\n",
    "        IN_FOLDER = '/media/allentao/One Touch/APS360/data/clean_data/'\n",
    "        file_path = \"../data/songs.json\"\n",
    "elif MODE == \"half\":\n",
    "        IN_FOLDER = '/media/allentao/One Touch/APS360/data/clean_data/'\n",
    "        file_path = \"../data/songs_med.json\" # half dataset\n",
    "elif MODE == \"small\":\n",
    "        IN_FOLDER = '../data/clean_data/'\n",
    "        file_path = \"../data/songs_small.json\"\n",
    "else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "        songs_file = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extend(data, max_length):\n",
    "#     new_data = []\n",
    "#     for i in range(len(data)):\n",
    "#         rows_needed = max_length - data[i][0].shape[0]\n",
    "#         zeros_to_add = torch.zeros((rows_needed, 128), dtype=data[i][0].dtype)\n",
    "#         new_song= torch.concatenate((data[i][0], zeros_to_add), axis=0)\n",
    "\n",
    "#         rows_needed = max_length - data[i][1].shape[0]\n",
    "#         zeros_to_add = torch.zeros((rows_needed, 128), dtype=data[i][1].dtype)\n",
    "#         new_cover= torch.concatenate((data[i][1], zeros_to_add), axis=0)\n",
    "        \n",
    "#         new_data.append((new_song, new_cover))\n",
    "#     return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    training_data = []\n",
    "    validation_data = []\n",
    "    testing_data = []\n",
    "\n",
    "    count = 0\n",
    "    max_length = 0\n",
    "    for song in songs_file[\"songs\"]:\n",
    "        song_file = song[\"filename\"]\n",
    "        song_num = int(song_file.split(\"_\")[0])\n",
    "        for piano_file in song[\"piano covers\"][\"filename\"]:\n",
    "            \n",
    "            name = os.path.splitext(piano_file)[0].split('_')[0] + \"_\" + os.path.splitext(piano_file)[0].split('_')[1]\n",
    "            song_file_path = IN_FOLDER + name + \"_song.midi\"\n",
    "            cover_file_path = IN_FOLDER + name + \"_cover.midi\"\n",
    "            print(\"Parsing\", song_file_path, cover_file_path)\n",
    "\n",
    "            song_piano_roll = midi_to_piano_roll(song_file_path)\n",
    "            cover_piano_roll = midi_to_piano_roll(cover_file_path)\n",
    "\n",
    "            if song_piano_roll == None or cover_piano_roll == None:\n",
    "                continue\n",
    "\n",
    "            song_piano_roll_val = song_piano_roll[:song_piano_roll.shape[-1]//2, :]\n",
    "            cover_piano_roll_val = cover_piano_roll[:cover_piano_roll.shape[-1]//2, :]\n",
    "            \n",
    "            song_length = song_piano_roll.shape[0]\n",
    "            cover_length = cover_piano_roll.shape[0]\n",
    "\n",
    "            if song_length > max_length:\n",
    "                max_length = song_length\n",
    "            if cover_length > max_length:\n",
    "                max_length = cover_length\n",
    "            training_data.append((song_piano_roll, cover_piano_roll))\n",
    "            if count < 200:\n",
    "                validation_data.append((song_piano_roll_val, cover_piano_roll_val))\n",
    "            elif count < 400:\n",
    "                testing_data.append((song_piano_roll_val, cover_piano_roll_val))\n",
    "            \n",
    "            print(\"Processed\", count, \"songs\")\n",
    "            count += 1\n",
    "\n",
    "    # training_data = extend(training_data, max_length)\n",
    "    # validation_data = extend(validation_data, max_length)\n",
    "    # testing_data = extend(testing_data, max_length)\n",
    "    \n",
    "    return training_data, validation_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paths():\n",
    "    # data will now consist of file paths, which can be loaded individually\n",
    "    # UPDATE: massive speedup, pickle the files on hdd, can load into memory in batches\n",
    "    #   save the file paths of the pkl's instead\n",
    "\n",
    "    training_data = []\n",
    "    validation_data = []\n",
    "    testing_data = []\n",
    "\n",
    "    count = 0\n",
    "    max_length = 0\n",
    "    for song in songs_file[\"songs\"]:\n",
    "        song_file = song[\"filename\"]\n",
    "        song_num = int(song_file.split(\"_\")[0])\n",
    "        for piano_file in song[\"piano covers\"][\"filename\"]:\n",
    "            \n",
    "            name = os.path.splitext(piano_file)[0].split('_')[0] + \"_\" + os.path.splitext(piano_file)[0].split('_')[1]\n",
    "            song_file_path = IN_FOLDER + name + \"_song.midi\"\n",
    "            cover_file_path = IN_FOLDER + name + \"_cover.midi\"\n",
    "            print(\"Parsing\", song_file_path, cover_file_path)\n",
    "\n",
    "            song_piano_roll = midi_to_piano_roll(song_file_path)\n",
    "            cover_piano_roll = midi_to_piano_roll(cover_file_path)\n",
    "\n",
    "            if song_piano_roll == None or cover_piano_roll == None:\n",
    "                continue\n",
    "\n",
    "            song_piano_roll_val = song_piano_roll[:song_piano_roll.shape[-1]//2, :]\n",
    "            cover_piano_roll_val = cover_piano_roll[:cover_piano_roll.shape[-1]//2, :]\n",
    "            \n",
    "            song_length = song_piano_roll.shape[0]\n",
    "            cover_length = cover_piano_roll.shape[0]\n",
    "\n",
    "            if song_length > max_length:\n",
    "                max_length = song_length\n",
    "            if cover_length > max_length:\n",
    "                max_length = cover_length\n",
    "\n",
    "            # pickle the data onto hdd\n",
    "            train_song_path = os.path.join(\"/media/allentao/One Touch/APS360/pkls/train\", name + \".npy\")\n",
    "            train_cover_path = os.path.join(\"/media/allentao/One Touch/APS360/pkls/train\", name + \".npy\")\n",
    "            np.save(train_song_path, song_piano_roll)\n",
    "            np.save(train_cover_path, cover_piano_roll)\n",
    "\n",
    "            # save some memory these npy files are like 20MB each\n",
    "            val_song_path = os.path.join(\"/media/allentao/One Touch/APS360/pkls/val\", name + \".npy\")\n",
    "            val_cover_path = os.path.join(\"/media/allentao/One Touch/APS360/pkls/val\", name + \".npy\")\n",
    "            if count < 400:\n",
    "                np.save(val_song_path, song_piano_roll_val)\n",
    "                np.save(val_cover_path, cover_piano_roll_val)\n",
    "\n",
    "            # save the file paths of the pkl's\n",
    "            training_data.append((train_song_path, train_cover_path))\n",
    "            if count < 200:\n",
    "                validation_data.append((val_song_path, val_cover_path))\n",
    "            elif count < 400:\n",
    "                testing_data.append((val_song_path, val_cover_path))\n",
    "            \n",
    "            print(\"Processed\", count, \"songs\")\n",
    "            count += 1\n",
    "\n",
    "    \n",
    "    return training_data, validation_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, lr, batch_size, training_data, validation_data, num_epochs, device, loss_func=\"mse\"):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    batch_size = batch_size\n",
    "    train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_total = 0.0\n",
    "        val_loss_total = 0.0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for data in train_loader:\n",
    "            count += 1\n",
    "            songs = data[0].to(device)\n",
    "\n",
    "            covers = data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(songs)\n",
    "            outputs = outputs.to(device)\n",
    "\n",
    "            # pad tensors to same length\n",
    "            if outputs.shape[1] > covers.shape[1]:\n",
    "                covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "            elif covers.shape[1] > outputs.shape[1]:\n",
    "                outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "            assert(outputs.shape == covers.shape)\n",
    "            \n",
    "            if loss_func == \"custom\":\n",
    "                loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "            elif loss_func == \"mse\":\n",
    "                loss = criterion(outputs, covers)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            loss.backward(retain_graph = True)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_total += loss.item()\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            # Add any other information you want to save (e.g., training loss, validation loss, etc.)\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, f'/media/allentao/One Touch/APS360/ckpts/checkpoint_epoch{epoch + 1}.pt')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in validation_loader:\n",
    "                # loss = criterion(outputs, labels)\n",
    "                songs = data[0].to(device)\n",
    "\n",
    "                covers = data[1].to(device)\n",
    "\n",
    "                outputs = model(songs)\n",
    "                outputs = outputs.to(device)\n",
    "\n",
    "                # pad tensors to same length\n",
    "                if outputs.shape[1] > covers.shape[1]:\n",
    "                    covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                elif covers.shape[1] > outputs.shape[1]:\n",
    "                    outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                assert(outputs.shape == covers.shape)\n",
    "                \n",
    "                if loss_func == \"custom\":\n",
    "                    loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                elif loss_func == \"mse\":\n",
    "                    loss = criterion(outputs, covers)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                \n",
    "                val_loss_total += loss.item()\n",
    "                \n",
    "        train_loss[epoch] = train_loss_total\n",
    "        val_loss[epoch] = val_loss_total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                f'Train Loss: {train_loss_total:.7f}, Train Loss: {train_loss_total:.7f}, '\n",
    "                f'Val Loss: {val_loss_total:.7f}, Val Loss: {val_loss_total:.7f}')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    model_path = str(lr) + '_' + str(batch_size) + '_' + str(num_epochs)\n",
    "    torch.save(model.state_dict(), 'model' + model_path)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch_sample(data, load_batch_size=1, dataset_type=\"train\"):\n",
    "\n",
    "    # should only be called in single data loading mode (to save memory)\n",
    "    # parses the midi file and returns the piano roll representation of the song and cover\n",
    "    # saves it in the correct format to be data loaded into pytorch\n",
    "\n",
    "    out_data = []\n",
    "\n",
    "    # print(\"TYPE:\", dataset_type)\n",
    "    # print(\"Building batch of size\", load_batch_size, \"...\")\n",
    "\n",
    "    for i in range(load_batch_size):\n",
    "\n",
    "        song_file_path = data[i][0]\n",
    "        cover_file_path = data[i][1]\n",
    "\n",
    "        # print(\"Loading: \", song_file_path, cover_file_path)\n",
    "        # input()\n",
    "\n",
    "        # song_piano_roll = midi_to_piano_roll(song_file_path)\n",
    "        # cover_piano_roll = midi_to_piano_roll(cover_file_path)\n",
    "\n",
    "        # if song_piano_roll == None or cover_piano_roll == None:\n",
    "        #     continue\n",
    "\n",
    "        # song_piano_roll_val = song_piano_roll[:song_piano_roll.shape[-1]//2, :]\n",
    "        # cover_piano_roll_val = cover_piano_roll[:cover_piano_roll.shape[-1]//2, :]\n",
    "\n",
    "        song_piano_roll = np.load(song_file_path)\n",
    "        cover_piano_roll = np.load(cover_file_path)\n",
    "        \n",
    "        song_length = song_piano_roll.shape[0]\n",
    "        cover_length = cover_piano_roll.shape[0]\n",
    "        \n",
    "        out_data.append((song_piano_roll, cover_piano_roll))\n",
    "\n",
    "\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_paths(model, lr, load_batch_size, training_data, validation_data, num_epochs, device, loss_func=\"mse\"):\n",
    "\n",
    "    # get current time\n",
    "    start = time.time()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "\n",
    "    batch_size = 1\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_total = 0.0\n",
    "        val_loss_total = 0.0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        data_path_batched = []\n",
    "\n",
    "        for i, data_path in enumerate(training_data):\n",
    "\n",
    "            data_path_batched.append(data_path)\n",
    "\n",
    "            if i % load_batch_size != 0 or i == 0: # only load when we accumulated enough\n",
    "                continue\n",
    "            \n",
    "            batch_data = prepare_batch_sample(data_path_batched, load_batch_size, \"train\")\n",
    "            train_loader = DataLoader(batch_data, batch_size=batch_size, shuffle=True) # this still has to be 1 due to vram constraints\n",
    "\n",
    "            for data in train_loader:\n",
    "                songs = data[0].to(device)\n",
    "\n",
    "                covers = data[1].to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(songs)\n",
    "                outputs = outputs.to(device)\n",
    "\n",
    "                # pad tensors to same length\n",
    "                if outputs.shape[1] > covers.shape[1]:\n",
    "                    covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                elif covers.shape[1] > outputs.shape[1]:\n",
    "                    outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                assert(outputs.shape == covers.shape)\n",
    "                \n",
    "                if loss_func == \"custom\":\n",
    "                    loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                elif loss_func == \"mse\":\n",
    "                    loss = criterion(outputs, covers)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "                loss.backward(retain_graph = True)\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss_total += loss.item()\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                # Add any other information you want to save (e.g., training loss, validation loss, etc.)\n",
    "            }\n",
    "\n",
    "            torch.save(checkpoint, f'/media/allentao/One Touch/APS360/ckpts/checkpoint_epoch{epoch + 1}.pt')\n",
    "\n",
    "            # reset\n",
    "            data_path_batched = []\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        data_path_batched = []\n",
    "        with torch.no_grad():\n",
    "            for i, data_path in enumerate(validation_data):\n",
    "                data_path_batched.append(data_path)\n",
    "\n",
    "                if i % load_batch_size != 0 or i == 0: # only load when we accumulated enough\n",
    "                    continue\n",
    "\n",
    "                batch_data = prepare_batch_sample(data_path_batched, load_batch_size, \"val\")\n",
    "                validation_loader = DataLoader(batch_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "                for data in validation_loader:\n",
    "\n",
    "                    songs = data[0].to(device)\n",
    "\n",
    "                    covers = data[1].to(device)\n",
    "\n",
    "                    outputs = model(songs)\n",
    "                    outputs = outputs.to(device)\n",
    "\n",
    "                    # pad tensors to same length\n",
    "                    if outputs.shape[1] > covers.shape[1]:\n",
    "                        covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                    elif covers.shape[1] > outputs.shape[1]:\n",
    "                        outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                    assert(outputs.shape == covers.shape)\n",
    "                    \n",
    "                    if loss_func == \"custom\":\n",
    "                        loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                    elif loss_func == \"mse\":\n",
    "                        loss = criterion(outputs, covers)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                    \n",
    "                    val_loss_total += loss.item()\n",
    "                    \n",
    "                data_path_batched = []\n",
    "                    \n",
    "        train_loss[epoch] = train_loss_total\n",
    "        val_loss[epoch] = val_loss_total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                f'Train Loss: {train_loss_total:.7f}, Train Loss: {train_loss_total:.7f}, '\n",
    "                f'Val Loss: {val_loss_total:.7f}, Val Loss: {val_loss_total:.7f}')\n",
    "        print(\"Time elapsed:\", time.time() - start)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    model_path = str(lr) + '_' + str(batch_size) + '_' + str(num_epochs)\n",
    "    torch.save(model.state_dict(), 'model' + model_path)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\" # force cpu\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(width = 3, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ../data/clean_data/0_0_song.midi ../data/clean_data/0_0_cover.midi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 songs\n",
      "Parsing ../data/clean_data/0_1_song.midi ../data/clean_data/0_1_cover.midi\n",
      "Processed 1 songs\n",
      "Parsing ../data/clean_data/0_2_song.midi ../data/clean_data/0_2_cover.midi\n",
      "Processed 2 songs\n",
      "Parsing ../data/clean_data/1_0_song.midi ../data/clean_data/1_0_cover.midi\n",
      "Processed 3 songs\n",
      "Parsing ../data/clean_data/1_1_song.midi ../data/clean_data/1_1_cover.midi\n",
      "Processed 4 songs\n",
      "Parsing ../data/clean_data/1_2_song.midi ../data/clean_data/1_2_cover.midi\n",
      "Processed 5 songs\n",
      "Parsing ../data/clean_data/2_0_song.midi ../data/clean_data/2_0_cover.midi\n",
      "Processed 6 songs\n",
      "Parsing ../data/clean_data/2_1_song.midi ../data/clean_data/2_1_cover.midi\n",
      "Processed 7 songs\n",
      "Parsing ../data/clean_data/2_2_song.midi ../data/clean_data/2_2_cover.midi\n",
      "Processed 8 songs\n",
      "Parsing ../data/clean_data/3_0_song.midi ../data/clean_data/3_0_cover.midi\n",
      "Parsing ../data/clean_data/3_1_song.midi ../data/clean_data/3_1_cover.midi\n",
      "Processed 9 songs\n",
      "Parsing ../data/clean_data/3_2_song.midi ../data/clean_data/3_2_cover.midi\n",
      "Processed 10 songs\n",
      "Parsing ../data/clean_data/3_3_song.midi ../data/clean_data/3_3_cover.midi\n",
      "Processed 11 songs\n",
      "Parsing ../data/clean_data/4_0_song.midi ../data/clean_data/4_0_cover.midi\n",
      "Processed 12 songs\n",
      "Parsing ../data/clean_data/4_1_song.midi ../data/clean_data/4_1_cover.midi\n",
      "Processed 13 songs\n",
      "Parsing ../data/clean_data/4_2_song.midi ../data/clean_data/4_2_cover.midi\n",
      "Processed 14 songs\n",
      "Parsing ../data/clean_data/5_0_song.midi ../data/clean_data/5_0_cover.midi\n",
      "Parsing ../data/clean_data/5_1_song.midi ../data/clean_data/5_1_cover.midi\n",
      "Processed 15 songs\n",
      "Parsing ../data/clean_data/6_0_song.midi ../data/clean_data/6_0_cover.midi\n",
      "Processed 16 songs\n",
      "Parsing ../data/clean_data/6_1_song.midi ../data/clean_data/6_1_cover.midi\n",
      "Processed 17 songs\n",
      "Parsing ../data/clean_data/6_2_song.midi ../data/clean_data/6_2_cover.midi\n",
      "Processed 18 songs\n",
      "Parsing ../data/clean_data/6_3_song.midi ../data/clean_data/6_3_cover.midi\n",
      "Processed 19 songs\n",
      "Parsing ../data/clean_data/6_4_song.midi ../data/clean_data/6_4_cover.midi\n",
      "Processed 20 songs\n"
     ]
    }
   ],
   "source": [
    "# WARNING: on single you only need to run this ONCE!\n",
    "if LOADER == \"batch\":\n",
    "    training_data, validation_data, testing_data = get_data()\n",
    "elif LOADER == \"single\" and not NOT_YET_LOADED:\n",
    "    training_data, validation_data, testing_data = get_data_paths()\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_loader:\n",
    "#     model = model.to(device)\n",
    "#     out = model(data[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 0.1407512, Train Loss: 0.1407512, Val Loss: 0.0121241, Val Loss: 0.0121241\n",
      "Time elapsed: 5.7174201011657715\n",
      "Epoch [2/200], Train Loss: 0.1407512, Train Loss: 0.1407512, Val Loss: 0.0120954, Val Loss: 0.0120954\n",
      "Time elapsed: 9.465549945831299\n",
      "Epoch [3/200], Train Loss: 0.1407512, Train Loss: 0.1407512, Val Loss: 0.0120700, Val Loss: 0.0120700\n",
      "Time elapsed: 13.170256614685059\n",
      "Epoch [4/200], Train Loss: 0.1407511, Train Loss: 0.1407511, Val Loss: 0.0120444, Val Loss: 0.0120444\n",
      "Time elapsed: 16.912707090377808\n",
      "Epoch [5/200], Train Loss: 0.1407512, Train Loss: 0.1407512, Val Loss: 0.0120226, Val Loss: 0.0120226\n",
      "Time elapsed: 20.651748180389404\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     model_train(model, \u001b[39m0.1\u001b[39m, BATCH_SIZE, training_data, validation_data, \u001b[39m200\u001b[39m, device, loss_func\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39melif\u001b[39;00m LOADER \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msingle\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     model_train_paths(model, \u001b[39m0.01\u001b[39;49m, LOAD_BATCH_SIZE, training_data, validation_data, \u001b[39m200\u001b[39;49m, device, loss_func\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmse\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 65\u001b[0m, in \u001b[0;36mmodel_train_paths\u001b[0;34m(model, lr, load_batch_size, training_data, validation_data, num_epochs, device, loss_func)\u001b[0m\n\u001b[1;32m     61\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     63\u001b[0m     train_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> 65\u001b[0m     torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mempty_cache()\n\u001b[1;32m     67\u001b[0m checkpoint \u001b[39m=\u001b[39m {\n\u001b[1;32m     68\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     69\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m     70\u001b[0m     \u001b[39m'\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m'\u001b[39m: optimizer\u001b[39m.\u001b[39mstate_dict(),\n\u001b[1;32m     71\u001b[0m     \u001b[39m# Add any other information you want to save (e.g., training loss, validation loss, etc.)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m }\n\u001b[1;32m     74\u001b[0m torch\u001b[39m.\u001b[39msave(checkpoint, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/media/allentao/One Touch/APS360/ckpts/checkpoint_epoch\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Melogen/aps360/lib/python3.8/site-packages/torch/cuda/memory.py:133\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 133\u001b[0m     torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_emptyCache()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if LOADER == \"batch\":\n",
    "    model_train(model, 0.1, BATCH_SIZE, training_data, validation_data, 200, device, loss_func=\"mse\")\n",
    "elif LOADER == \"single\":\n",
    "    model_train_paths(model, 0.01, LOAD_BATCH_SIZE, training_data, validation_data, 200, device, loss_func=\"mse\")\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aps360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
