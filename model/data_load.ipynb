{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from USB\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pretty_midi\n",
    "from model import Net\n",
    "from midi_to_piano_roll import midi_to_piano_roll\n",
    "from loss import blur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code on small dataset, or on full dataset\n",
    "mode = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"full\":\n",
    "        IN_FOLDER = '/media/allentao/One Touch/APS360/data/clean_data/'\n",
    "        file_path = \"../data/songs.json\"\n",
    "elif mode == \"small\":\n",
    "        IN_FOLDER = '../data/clean_data/'\n",
    "        file_path = \"../data/songs_small.json\"\n",
    "else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "        songs_file = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend(data, max_length):\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        rows_needed = max_length - data[i][0].shape[0]\n",
    "        zeros_to_add = torch.zeros((rows_needed, 128), dtype=data[i][0].dtype)\n",
    "        new_song= torch.concatenate((data[i][0], zeros_to_add), axis=0)\n",
    "\n",
    "        rows_needed = max_length - data[i][1].shape[0]\n",
    "        zeros_to_add = torch.zeros((rows_needed, 128), dtype=data[i][1].dtype)\n",
    "        new_cover= torch.concatenate((data[i][1], zeros_to_add), axis=0)\n",
    "        \n",
    "        new_data.append((new_song, new_cover))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    training_data = []\n",
    "    validation_data = []\n",
    "    testing_data = []\n",
    "\n",
    "    count = 0\n",
    "    max_length = 0\n",
    "    for song in songs_file[\"songs\"]:\n",
    "        song_file = song[\"filename\"]\n",
    "        song_num = int(song_file.split(\"_\")[0])\n",
    "        for piano_file in song[\"piano covers\"][\"filename\"]:\n",
    "            \n",
    "            name = os.path.splitext(piano_file)[0].split('_')[0] + \"_\" + os.path.splitext(piano_file)[0].split('_')[1]\n",
    "            song_file_path = IN_FOLDER + name + \"_song.midi\"\n",
    "            cover_file_path = IN_FOLDER + name + \"_cover.midi\"\n",
    "            print(\"Parsing\", song_file_path, cover_file_path)\n",
    "\n",
    "            song_piano_roll = midi_to_piano_roll(song_file_path)\n",
    "            cover_piano_roll = midi_to_piano_roll(cover_file_path)\n",
    "\n",
    "            if song_piano_roll == None or cover_piano_roll == None:\n",
    "                continue\n",
    "\n",
    "            song_piano_roll_val = song_piano_roll[:song_piano_roll.shape[-1]//2, :]\n",
    "            cover_piano_roll_val = cover_piano_roll[:cover_piano_roll.shape[-1]//2, :]\n",
    "            \n",
    "            song_length = song_piano_roll.shape[0]\n",
    "            cover_length = cover_piano_roll.shape[0]\n",
    "\n",
    "            if song_length > max_length:\n",
    "                max_length = song_length\n",
    "            if cover_length > max_length:\n",
    "                max_length = cover_length\n",
    "            training_data.append((song_piano_roll, cover_piano_roll))\n",
    "            if count < 200:\n",
    "                validation_data.append((song_piano_roll_val, cover_piano_roll_val))\n",
    "            elif count < 400:\n",
    "                testing_data.append((song_piano_roll_val, cover_piano_roll_val))\n",
    "            \n",
    "            print(\"Processed\", count, \"songs\")\n",
    "            count += 1\n",
    "\n",
    "    training_data = extend(training_data, max_length)\n",
    "    validation_data = extend(validation_data, max_length)\n",
    "    testing_data = extend(testing_data, max_length)\n",
    "    \n",
    "    return training_data, validation_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, lr, batch_size, training_data, validation_data, num_epochs, device, loss_func=\"mse\"):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    batch_size = batch_size\n",
    "    train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_total = 0.0\n",
    "        val_loss_total = 0.0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for data in train_loader:\n",
    "            count += 1\n",
    "            songs = data[0].to(device)\n",
    "\n",
    "            covers = data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(songs)\n",
    "            outputs = outputs.to(device)\n",
    "\n",
    "            # pad tensors to same length\n",
    "            if outputs.shape[1] > covers.shape[1]:\n",
    "                covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "            elif covers.shape[1] > outputs.shape[1]:\n",
    "                outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "            assert(outputs.shape == covers.shape)\n",
    "            \n",
    "            if loss_func == \"custom\":\n",
    "                loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "            elif loss_func == \"mse\":\n",
    "                loss = criterion(outputs, covers)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            loss.backward(retain_graph = True)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_total += loss.item()\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            # Add any other information you want to save (e.g., training loss, validation loss, etc.)\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, f'/media/allentao/One Touch/APS360/ckpts/checkpoint_epoch{epoch + 1}.pt')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in validation_loader:\n",
    "                # loss = criterion(outputs, labels)\n",
    "                songs = data[0].to(device)\n",
    "\n",
    "                covers = data[1].to(device)\n",
    "\n",
    "                outputs = model(songs)\n",
    "                outputs = outputs.to(device)\n",
    "\n",
    "                # pad tensors to same length\n",
    "                if outputs.shape[1] > covers.shape[1]:\n",
    "                    covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                elif covers.shape[1] > outputs.shape[1]:\n",
    "                    outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                assert(outputs.shape == covers.shape)\n",
    "                \n",
    "                if loss_func == \"custom\":\n",
    "                    loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                elif loss_func == \"mse\":\n",
    "                    loss = criterion(outputs, covers)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                \n",
    "                val_loss_total += loss.item()\n",
    "                \n",
    "        train_loss[epoch] = train_loss_total\n",
    "        val_loss[epoch] = val_loss_total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                f'Train Loss: {train_loss_total:.7f}, Train Loss: {train_loss_total:.7f}, '\n",
    "                f'Val Loss: {val_loss_total:.7f}, Val Loss: {val_loss_total:.7f}')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    model_path = str(lr) + '_' + str(batch_size) + '_' + str(num_epochs)\n",
    "    torch.save(model.state_dict(), 'model' + model_path)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\" # force cpu\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(width = 3, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/clean_data/\n",
      "../data/songs_small.json\n",
      "Parsing ../data/clean_data/0_0_song.midi ../data/clean_data/0_0_cover.midi\n",
      "Processed 0 songs\n",
      "Parsing ../data/clean_data/0_1_song.midi ../data/clean_data/0_1_cover.midi\n",
      "Processed 1 songs\n",
      "Parsing ../data/clean_data/0_2_song.midi ../data/clean_data/0_2_cover.midi\n",
      "Processed 2 songs\n",
      "Parsing ../data/clean_data/1_0_song.midi ../data/clean_data/1_0_cover.midi\n",
      "Processed 3 songs\n",
      "Parsing ../data/clean_data/1_1_song.midi ../data/clean_data/1_1_cover.midi\n",
      "Processed 4 songs\n",
      "Parsing ../data/clean_data/1_2_song.midi ../data/clean_data/1_2_cover.midi\n",
      "Processed 5 songs\n",
      "Parsing ../data/clean_data/2_0_song.midi ../data/clean_data/2_0_cover.midi\n",
      "Processed 6 songs\n",
      "Parsing ../data/clean_data/2_1_song.midi ../data/clean_data/2_1_cover.midi\n",
      "Processed 7 songs\n",
      "Parsing ../data/clean_data/2_2_song.midi ../data/clean_data/2_2_cover.midi\n",
      "Processed 8 songs\n",
      "Parsing ../data/clean_data/3_0_song.midi ../data/clean_data/3_0_cover.midi\n",
      "Parsing ../data/clean_data/3_1_song.midi ../data/clean_data/3_1_cover.midi\n",
      "Processed 9 songs\n",
      "Parsing ../data/clean_data/3_2_song.midi ../data/clean_data/3_2_cover.midi\n",
      "Processed 10 songs\n",
      "Parsing ../data/clean_data/3_3_song.midi ../data/clean_data/3_3_cover.midi\n",
      "Processed 11 songs\n",
      "Parsing ../data/clean_data/4_0_song.midi ../data/clean_data/4_0_cover.midi\n",
      "Processed 12 songs\n",
      "Parsing ../data/clean_data/4_1_song.midi ../data/clean_data/4_1_cover.midi\n",
      "Processed 13 songs\n",
      "Parsing ../data/clean_data/4_2_song.midi ../data/clean_data/4_2_cover.midi\n",
      "Processed 14 songs\n",
      "Parsing ../data/clean_data/5_0_song.midi ../data/clean_data/5_0_cover.midi\n",
      "Parsing ../data/clean_data/5_1_song.midi ../data/clean_data/5_1_cover.midi\n",
      "Processed 15 songs\n",
      "Parsing ../data/clean_data/6_0_song.midi ../data/clean_data/6_0_cover.midi\n",
      "Processed 16 songs\n",
      "Parsing ../data/clean_data/6_1_song.midi ../data/clean_data/6_1_cover.midi\n",
      "Processed 17 songs\n",
      "Parsing ../data/clean_data/6_2_song.midi ../data/clean_data/6_2_cover.midi\n",
      "Processed 18 songs\n",
      "Parsing ../data/clean_data/6_3_song.midi ../data/clean_data/6_3_cover.midi\n",
      "Processed 19 songs\n",
      "Parsing ../data/clean_data/6_4_song.midi ../data/clean_data/6_4_cover.midi\n",
      "Processed 20 songs\n"
     ]
    }
   ],
   "source": [
    "print(IN_FOLDER)\n",
    "print(file_path)\n",
    "training_data, validation_data, testing_data = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_loader:\n",
    "#     model = model.to(device)\n",
    "#     out = model(data[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.1051636, Train Loss: 0.1051636, Val Loss: 0.0000211, Val Loss: 0.0000211\n",
      "Epoch [2/10], Train Loss: 0.1051655, Train Loss: 0.1051655, Val Loss: 0.0000216, Val Loss: 0.0000216\n",
      "Epoch [3/10], Train Loss: 0.1051591, Train Loss: 0.1051591, Val Loss: 0.0000248, Val Loss: 0.0000248\n",
      "Epoch [4/10], Train Loss: 0.1051567, Train Loss: 0.1051567, Val Loss: 0.0000272, Val Loss: 0.0000272\n",
      "Epoch [5/10], Train Loss: 0.1051585, Train Loss: 0.1051585, Val Loss: 0.0000275, Val Loss: 0.0000275\n",
      "Epoch [6/10], Train Loss: 0.1051604, Train Loss: 0.1051604, Val Loss: 0.0000212, Val Loss: 0.0000212\n",
      "Epoch [7/10], Train Loss: 0.1051686, Train Loss: 0.1051686, Val Loss: 0.0000239, Val Loss: 0.0000239\n",
      "Epoch [8/10], Train Loss: 0.1051696, Train Loss: 0.1051696, Val Loss: 0.0000289, Val Loss: 0.0000289\n",
      "Epoch [9/10], Train Loss: 0.1051726, Train Loss: 0.1051726, Val Loss: 0.0000211, Val Loss: 0.0000211\n",
      "Epoch [10/10], Train Loss: 0.1051730, Train Loss: 0.1051730, Val Loss: 0.0000212, Val Loss: 0.0000212\n"
     ]
    }
   ],
   "source": [
    "model_train(model, 1e5, batch_size, training_data, validation_data, 10, device, loss_func=\"mse\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aps360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
