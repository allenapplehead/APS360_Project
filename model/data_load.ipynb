{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from USB\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pretty_midi\n",
    "from model import Net\n",
    "from midi_to_piano_roll import midi_to_piano_roll\n",
    "from loss import blur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code on small dataset, or on full dataset\n",
    "mode = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"full\":\n",
    "        IN_FOLDER = '/media/allentao/One Touch/APS360/data/clean_data/'\n",
    "        file_path = \"../data/songs.json\" # full dataset is too large to load into memory\n",
    "elif mode == \"small\":\n",
    "        IN_FOLDER = '../data/clean_data/'\n",
    "        file_path = \"../data/songs_small.json\"\n",
    "else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "        songs_file = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend(data, max_length):\n",
    "    new_data = []\n",
    "    for i in range(len(data)):\n",
    "        rows_needed = max_length - data[i][0].shape[0]\n",
    "        zeros_to_add = torch.zeros((rows_needed, 128), dtype=data[i][0].dtype)\n",
    "        new_song= torch.concatenate((data[i][0], zeros_to_add), axis=0)\n",
    "\n",
    "        rows_needed = max_length - data[i][1].shape[0]\n",
    "        zeros_to_add = torch.zeros((rows_needed, 128), dtype=data[i][1].dtype)\n",
    "        new_cover= torch.concatenate((data[i][1], zeros_to_add), axis=0)\n",
    "        \n",
    "        new_data.append((new_song, new_cover))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    training_data = []\n",
    "    validation_data = []\n",
    "    testing_data = []\n",
    "\n",
    "    count = 0\n",
    "    max_length = 0\n",
    "    for song in songs_file[\"songs\"]:\n",
    "        song_file = song[\"filename\"]\n",
    "        song_num = int(song_file.split(\"_\")[0])\n",
    "        for piano_file in song[\"piano covers\"][\"filename\"]:\n",
    "            \n",
    "            name = os.path.splitext(piano_file)[0].split('_')[0] + \"_\" + os.path.splitext(piano_file)[0].split('_')[1]\n",
    "            song_file_path = IN_FOLDER + name + \"_song.midi\"\n",
    "            cover_file_path = IN_FOLDER + name + \"_cover.midi\"\n",
    "            print(\"Parsing\", song_file_path, cover_file_path)\n",
    "\n",
    "            song_piano_roll = midi_to_piano_roll(song_file_path)\n",
    "            cover_piano_roll = midi_to_piano_roll(cover_file_path)\n",
    "\n",
    "            if song_piano_roll == None or cover_piano_roll == None:\n",
    "                continue\n",
    "\n",
    "            song_piano_roll_val = song_piano_roll[:song_piano_roll.shape[-1]//2, :]\n",
    "            cover_piano_roll_val = cover_piano_roll[:cover_piano_roll.shape[-1]//2, :]\n",
    "            \n",
    "            song_length = song_piano_roll.shape[0]\n",
    "            cover_length = cover_piano_roll.shape[0]\n",
    "\n",
    "            if song_length > max_length:\n",
    "                max_length = song_length\n",
    "            if cover_length > max_length:\n",
    "                max_length = cover_length\n",
    "            training_data.append((song_piano_roll, cover_piano_roll))\n",
    "            if count < 200:\n",
    "                validation_data.append((song_piano_roll_val, cover_piano_roll_val))\n",
    "            elif count < 400:\n",
    "                testing_data.append((song_piano_roll_val, cover_piano_roll_val))\n",
    "            \n",
    "            print(\"Processed\", count, \"songs\")\n",
    "            count += 1\n",
    "\n",
    "    # training_data = extend(training_data, max_length)\n",
    "    # validation_data = extend(validation_data, max_length)\n",
    "    # testing_data = extend(testing_data, max_length)\n",
    "    \n",
    "    return training_data, validation_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, lr, batch_size, training_data, validation_data, num_epochs, device, loss_func=\"mse\"):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    batch_size = batch_size\n",
    "    train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_total = 0.0\n",
    "        val_loss_total = 0.0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for data in train_loader:\n",
    "            count += 1\n",
    "            songs = data[0].to(device)\n",
    "\n",
    "            covers = data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(songs)\n",
    "            outputs = outputs.to(device)\n",
    "\n",
    "            # pad tensors to same length\n",
    "            if outputs.shape[1] > covers.shape[1]:\n",
    "                covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "            elif covers.shape[1] > outputs.shape[1]:\n",
    "                outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "            assert(outputs.shape == covers.shape)\n",
    "            \n",
    "            if loss_func == \"custom\":\n",
    "                loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "            elif loss_func == \"mse\":\n",
    "                loss = criterion(outputs, covers)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            loss.backward(retain_graph = True)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_total += loss.item()\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            # Add any other information you want to save (e.g., training loss, validation loss, etc.)\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, f'/media/allentao/One Touch/APS360/ckpts/checkpoint_epoch{epoch + 1}.pt')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in validation_loader:\n",
    "                # loss = criterion(outputs, labels)\n",
    "                songs = data[0].to(device)\n",
    "\n",
    "                covers = data[1].to(device)\n",
    "\n",
    "                outputs = model(songs)\n",
    "                outputs = outputs.to(device)\n",
    "\n",
    "                # pad tensors to same length\n",
    "                if outputs.shape[1] > covers.shape[1]:\n",
    "                    covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                elif covers.shape[1] > outputs.shape[1]:\n",
    "                    outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                assert(outputs.shape == covers.shape)\n",
    "                \n",
    "                if loss_func == \"custom\":\n",
    "                    loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                elif loss_func == \"mse\":\n",
    "                    loss = criterion(outputs, covers)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                \n",
    "                val_loss_total += loss.item()\n",
    "                \n",
    "        train_loss[epoch] = train_loss_total\n",
    "        val_loss[epoch] = val_loss_total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                f'Train Loss: {train_loss_total:.7f}, Train Loss: {train_loss_total:.7f}, '\n",
    "                f'Val Loss: {val_loss_total:.7f}, Val Loss: {val_loss_total:.7f}')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    model_path = str(lr) + '_' + str(batch_size) + '_' + str(num_epochs)\n",
    "    torch.save(model.state_dict(), 'model' + model_path)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\" # force cpu\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(width = 3, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/allentao/One Touch/APS360/data/clean_data/\n",
      "../data/songs_med.json\n",
      "Parsing /media/allentao/One Touch/APS360/data/clean_data/0_0_song.midi /media/allentao/One Touch/APS360/data/clean_data/0_0_cover.midi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 songs\n",
      "Parsing /media/allentao/One Touch/APS360/data/clean_data/0_1_song.midi /media/allentao/One Touch/APS360/data/clean_data/0_1_cover.midi\n",
      "Processed 1 songs\n",
      "Parsing /media/allentao/One Touch/APS360/data/clean_data/0_2_song.midi /media/allentao/One Touch/APS360/data/clean_data/0_2_cover.midi\n",
      "Parsing /media/allentao/One Touch/APS360/data/clean_data/1_0_song.midi /media/allentao/One Touch/APS360/data/clean_data/1_0_cover.midi\n",
      "Processed 2 songs\n",
      "Parsing /media/allentao/One Touch/APS360/data/clean_data/1_1_song.midi /media/allentao/One Touch/APS360/data/clean_data/1_1_cover.midi\n",
      "Processed 3 songs\n",
      "Parsing /media/allentao/One Touch/APS360/data/clean_data/1_2_song.midi /media/allentao/One Touch/APS360/data/clean_data/1_2_cover.midi\n",
      "Processed 4 songs\n",
      "Parsing /media/allentao/One Touch/APS360/data/clean_data/2_0_song.midi /media/allentao/One Touch/APS360/data/clean_data/2_0_cover.midi\n",
      "Processed 5 songs\n",
      "Parsing /media/allentao/One Touch/APS360/data/clean_data/2_1_song.midi /media/allentao/One Touch/APS360/data/clean_data/2_1_cover.midi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(IN_FOLDER)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(file_path)\n\u001b[0;32m----> 3\u001b[0m training_data, validation_data, testing_data \u001b[39m=\u001b[39m get_data()\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mParsing\u001b[39m\u001b[39m\"\u001b[39m, song_file_path, cover_file_path)\n\u001b[1;32m     18\u001b[0m song_piano_roll \u001b[39m=\u001b[39m midi_to_piano_roll(song_file_path)\n\u001b[0;32m---> 19\u001b[0m cover_piano_roll \u001b[39m=\u001b[39m midi_to_piano_roll(cover_file_path)\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m song_piano_roll \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m cover_piano_roll \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Melogen/model/midi_to_piano_roll.py:37\u001b[0m, in \u001b[0;36mmidi_to_piano_roll\u001b[0;34m(midi_file_path, time_resolution)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(piano_roll)):\n\u001b[1;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(piano_roll[i])):\n\u001b[0;32m---> 37\u001b[0m         \u001b[39msum\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m piano_roll[i][j]\n\u001b[1;32m     39\u001b[0m piano_roll \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(piano_roll)\n\u001b[1;32m     40\u001b[0m piano_roll \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39mtensor(piano_roll, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(IN_FOLDER)\n",
    "print(file_path)\n",
    "training_data, validation_data, testing_data = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_loader:\n",
    "#     model = model.to(device)\n",
    "#     out = model(data[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.1448535, Train Loss: 0.1448535, Val Loss: 0.0238449, Val Loss: 0.0238449\n",
      "Epoch [2/10], Train Loss: 0.1448556, Train Loss: 0.1448556, Val Loss: 0.0226380, Val Loss: 0.0226380\n",
      "Epoch [3/10], Train Loss: 0.1448541, Train Loss: 0.1448541, Val Loss: 0.0205448, Val Loss: 0.0205448\n",
      "Epoch [4/10], Train Loss: 0.1448511, Train Loss: 0.1448511, Val Loss: 0.0229805, Val Loss: 0.0229805\n",
      "Epoch [5/10], Train Loss: 0.1448465, Train Loss: 0.1448465, Val Loss: 0.0281214, Val Loss: 0.0281214\n",
      "Epoch [6/10], Train Loss: 0.1448440, Train Loss: 0.1448440, Val Loss: 0.0344856, Val Loss: 0.0344856\n",
      "Epoch [7/10], Train Loss: 0.1448424, Train Loss: 0.1448424, Val Loss: 0.0192781, Val Loss: 0.0192781\n",
      "Epoch [8/10], Train Loss: 0.1448418, Train Loss: 0.1448418, Val Loss: 0.0356231, Val Loss: 0.0356231\n",
      "Epoch [9/10], Train Loss: 0.1448443, Train Loss: 0.1448443, Val Loss: 0.0210562, Val Loss: 0.0210562\n",
      "Epoch [10/10], Train Loss: 0.1448433, Train Loss: 0.1448433, Val Loss: 0.0282223, Val Loss: 0.0282223\n"
     ]
    }
   ],
   "source": [
    "model_train(model, 1e4, batch_size, training_data, validation_data, 10, device, loss_func=\"mse\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aps360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
