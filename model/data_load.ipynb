{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 𝄆  Melogen Training Script  𝄇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from USB\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pretty_midi\n",
    "from model import Net\n",
    "from midi_to_piano_roll import midi_to_piano_roll, postprocess\n",
    "from loss import blur_loss\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIGS ###\n",
    "\n",
    "# Test code on small dataset, half dataset, or on full dataset\n",
    "MODE = \"full\"\n",
    "\n",
    "# Set the batch size\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Set loader style\n",
    "LOADER = \"single\"\n",
    "LOAD_BATCH_SIZE = 250\n",
    "\n",
    "# Train, val, test split\n",
    "VAL_SAMPLES = 200\n",
    "TEST_SAMPLES = 400\n",
    "\n",
    "# If we already saved the npy's no need to resave\n",
    "ALREADY_LOADED = True\n",
    "\n",
    "CKPT_PATH = \"/media/allentao/One Touch/APS360/ckpts/aug1lr0.0001overfit\"\n",
    "# CKPT_PATH = None\n",
    "\n",
    "# Set to train mode or evaluation mode\n",
    "SETTING = \"eval\" # \"train\", \"eval\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"full\":\n",
    "        IN_FOLDER = '/media/allentao/One Touch/APS360/data/clean_data/'\n",
    "        file_path = \"../data/songs.json\"\n",
    "elif MODE == \"half\":\n",
    "        IN_FOLDER = '/media/allentao/One Touch/APS360/data/clean_data/'\n",
    "        file_path = \"../data/songs_med.json\" # half dataset\n",
    "elif MODE == \"small\":\n",
    "        IN_FOLDER = '../data/clean_data/'\n",
    "        file_path = \"../data/songs_small.json\"\n",
    "else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "        songs_file = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extend(data, max_length):\n",
    "#     new_data = []\n",
    "#     for i in range(len(data)):\n",
    "#         rows_needed = max_length - data[i][0].shape[0]\n",
    "#         zeros_to_add = torch.zeros((rows_needed, 128), dtype=data[i][0].dtype)\n",
    "#         new_song= torch.concatenate((data[i][0], zeros_to_add), axis=0)\n",
    "\n",
    "#         rows_needed = max_length - data[i][1].shape[0]\n",
    "#         zeros_to_add = torch.zeros((rows_needed, 128), dtype=data[i][1].dtype)\n",
    "#         new_cover= torch.concatenate((data[i][1], zeros_to_add), axis=0)\n",
    "        \n",
    "#         new_data.append((new_song, new_cover))\n",
    "#     return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    training_data = []\n",
    "    validation_data = []\n",
    "    testing_data = []\n",
    "\n",
    "    count = 0\n",
    "    max_length = 0\n",
    "    for song in songs_file[\"songs\"]:\n",
    "        song_file = song[\"filename\"]\n",
    "        song_num = int(song_file.split(\"_\")[0])\n",
    "        for piano_file in song[\"piano covers\"][\"filename\"]:\n",
    "            \n",
    "            name = os.path.splitext(piano_file)[0].split('_')[0] + \"_\" + os.path.splitext(piano_file)[0].split('_')[1]\n",
    "            song_file_path = IN_FOLDER + name + \"_song.midi\"\n",
    "            cover_file_path = IN_FOLDER + name + \"_cover.midi\"\n",
    "            print(\"Parsing\", song_file_path, cover_file_path)\n",
    "\n",
    "            song_piano_roll = midi_to_piano_roll(song_file_path)\n",
    "            cover_piano_roll = midi_to_piano_roll(cover_file_path)\n",
    "\n",
    "            if song_piano_roll == None or cover_piano_roll == None:\n",
    "                continue\n",
    "\n",
    "            song_piano_roll_val = song_piano_roll[:song_piano_roll.shape[-1]//2, :]\n",
    "            cover_piano_roll_val = cover_piano_roll[:cover_piano_roll.shape[-1]//2, :]\n",
    "            \n",
    "            song_length = song_piano_roll.shape[0]\n",
    "            cover_length = cover_piano_roll.shape[0]\n",
    "\n",
    "            if song_length > max_length:\n",
    "                max_length = song_length\n",
    "            if cover_length > max_length:\n",
    "                max_length = cover_length\n",
    "            if count < VAL_SAMPLES:\n",
    "                validation_data.append((song_piano_roll_val, cover_piano_roll_val))\n",
    "            elif VAL_SAMPLES <= count < TEST_SAMPLES:\n",
    "                testing_data.append((song_piano_roll_val, cover_piano_roll_val))\n",
    "            else:\n",
    "                training_data.append((song_piano_roll, cover_piano_roll))\n",
    "                \n",
    "            print(\"Processed\", count, \"songs\")\n",
    "            count += 1\n",
    "\n",
    "    # training_data = extend(training_data, max_length)\n",
    "    # validation_data = extend(validation_data, max_length)\n",
    "    # testing_data = extend(testing_data, max_length)\n",
    "    \n",
    "    return training_data, validation_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paths(already_loaded=False):\n",
    "    # data will now consist of file paths, which can be loaded individually\n",
    "    # UPDATE: massive speedup, pickle the files on hdd, can load into memory in batches\n",
    "    #   save the file paths of the pkl's instead\n",
    "\n",
    "    training_data = []\n",
    "    validation_data = []\n",
    "    testing_data = []\n",
    "\n",
    "    count = 0\n",
    "    max_length = 0\n",
    "    for song in songs_file[\"songs\"]:\n",
    "        song_file = song[\"filename\"]\n",
    "        song_num = int(song_file.split(\"_\")[0])\n",
    "        for piano_file in song[\"piano covers\"][\"filename\"]:\n",
    "            \n",
    "            name = os.path.splitext(piano_file)[0].split('_')[0] + \"_\" + os.path.splitext(piano_file)[0].split('_')[1]\n",
    "            song_file_path = IN_FOLDER + name + \"_song.midi\"\n",
    "            cover_file_path = IN_FOLDER + name + \"_cover.midi\"\n",
    "            print(\"Parsing\", song_file_path, cover_file_path)\n",
    "\n",
    "            train_song_path = os.path.join(\"/media/allentao/One Touch/APS360/pkls/train\", name + \".npy\")\n",
    "            train_cover_path = os.path.join(\"/media/allentao/One Touch/APS360/pkls/train\", name + \".npy\")\n",
    "\n",
    "            val_song_path = os.path.join(\"/media/allentao/One Touch/APS360/pkls/val\", name + \".npy\")\n",
    "            val_cover_path = os.path.join(\"/media/allentao/One Touch/APS360/pkls/val\", name + \".npy\")\n",
    "\n",
    "            if not already_loaded:\n",
    "                song_piano_roll = midi_to_piano_roll(song_file_path)\n",
    "                cover_piano_roll = midi_to_piano_roll(cover_file_path)\n",
    "\n",
    "                if song_piano_roll == None or cover_piano_roll == None:\n",
    "                    continue\n",
    "\n",
    "                song_piano_roll_val = song_piano_roll[:song_piano_roll.shape[-1]//2, :]\n",
    "                cover_piano_roll_val = cover_piano_roll[:cover_piano_roll.shape[-1]//2, :]\n",
    "                \n",
    "                song_length = song_piano_roll.shape[0]\n",
    "                cover_length = cover_piano_roll.shape[0]\n",
    "\n",
    "                if song_length > max_length:\n",
    "                    max_length = song_length\n",
    "                if cover_length > max_length:\n",
    "                    max_length = cover_length\n",
    "\n",
    "                # pickle the data onto hdd\n",
    "                \n",
    "                np.save(train_song_path, song_piano_roll)\n",
    "                np.save(train_cover_path, cover_piano_roll)\n",
    "\n",
    "                # save some memory these npy files are like 20MB each\n",
    "                \n",
    "                if count < max(TEST_SAMPLES, VAL_SAMPLES):\n",
    "                    np.save(val_song_path, song_piano_roll_val)\n",
    "                    np.save(val_cover_path, cover_piano_roll_val)\n",
    "            else:\n",
    "                # append if file path exists\n",
    "                if not os.path.exists(train_song_path) or not os.path.exists(train_cover_path):\n",
    "                    continue\n",
    "\n",
    "                # save the file paths of the pkl's\n",
    "                training_data.append((train_song_path, train_cover_path))\n",
    "                if count < VAL_SAMPLES:\n",
    "                    # validation_data.append((val_song_path, val_cover_path))\n",
    "                    validation_data.append((train_song_path, train_cover_path))\n",
    "                elif count < TEST_SAMPLES:\n",
    "                    # testing_data.append((val_song_path, val_cover_path))\n",
    "                    testing_data.append((train_song_path, train_cover_path))\n",
    "            \n",
    "            print(\"Processed\", count, \"songs\")\n",
    "            count += 1\n",
    "\n",
    "    \n",
    "    return training_data, validation_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, lr, batch_size, training_data, validation_data, num_epochs, device, loss_func=\"mse\"):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    batch_size = batch_size\n",
    "    train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_total = 0.0\n",
    "        val_loss_total = 0.0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        count = 0\n",
    "        for data in train_loader:\n",
    "            count += 1\n",
    "            songs = data[0].to(device)\n",
    "\n",
    "            covers = data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(songs)\n",
    "            outputs = outputs.to(device)\n",
    "\n",
    "            # pad tensors to same length\n",
    "            if outputs.shape[1] > covers.shape[1]:\n",
    "                covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "            elif covers.shape[1] > outputs.shape[1]:\n",
    "                outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "            assert(outputs.shape == covers.shape)\n",
    "            \n",
    "            if loss_func == \"custom\":\n",
    "                loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "            elif loss_func == \"mse\":\n",
    "                loss = criterion(outputs, covers)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            loss.backward(retain_graph = True)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_total += loss.item()\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            # Add any other information you want to save (e.g., training loss, validation loss, etc.)\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, f'/media/allentao/One Touch/APS360/ckpts/checkpoint_epoch{epoch + 1}.pt')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in validation_loader:\n",
    "                # loss = criterion(outputs, labels)\n",
    "                songs = data[0].to(device)\n",
    "\n",
    "                covers = data[1].to(device)\n",
    "\n",
    "                outputs = model(songs)\n",
    "                outputs = outputs.to(device)\n",
    "\n",
    "                # pad tensors to same length\n",
    "                if outputs.shape[1] > covers.shape[1]:\n",
    "                    covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                elif covers.shape[1] > outputs.shape[1]:\n",
    "                    outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                assert(outputs.shape == covers.shape)\n",
    "                \n",
    "                if loss_func == \"custom\":\n",
    "                    loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                elif loss_func == \"mse\":\n",
    "                    loss = criterion(outputs, covers)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                \n",
    "                val_loss_total += loss.item()\n",
    "                \n",
    "        train_loss[epoch] = train_loss_total\n",
    "        val_loss[epoch] = val_loss_total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                f'Train Loss: {train_loss_total:.7f}, Train Loss: {train_loss_total:.7f}, '\n",
    "                f'Val Loss: {val_loss_total:.7f}, Val Loss: {val_loss_total:.7f}')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    model_path = str(lr) + '_' + str(batch_size) + '_' + str(num_epochs)\n",
    "    torch.save(model.state_dict(), 'model' + model_path)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch_sample(data, load_batch_size=1, dataset_type=\"train\"):\n",
    "\n",
    "    # should only be called in single data loading mode (to save memory)\n",
    "    # parses the midi file and returns the piano roll representation of the song and cover\n",
    "    # saves it in the correct format to be data loaded into pytorch\n",
    "\n",
    "    out_data = []\n",
    "\n",
    "    # print(\"TYPE:\", dataset_type)\n",
    "    # print(\"Building batch of size\", load_batch_size, \"...\")\n",
    "    # print(\"DEBUG:\", len(data), load_batch_size)\n",
    "\n",
    "    for i in range(load_batch_size):\n",
    "\n",
    "        song_file_path = data[i][0]\n",
    "        cover_file_path = data[i][1]\n",
    "\n",
    "        # print(\"Loading: \", song_file_path, cover_file_path)\n",
    "        # input()\n",
    "\n",
    "        # song_piano_roll = midi_to_piano_roll(song_file_path)\n",
    "        # cover_piano_roll = midi_to_piano_roll(cover_file_path)\n",
    "\n",
    "        # if song_piano_roll == None or cover_piano_roll == None:\n",
    "        #     continue\n",
    "\n",
    "        # song_piano_roll_val = song_piano_roll[:song_piano_roll.shape[-1]//2, :]\n",
    "        # cover_piano_roll_val = cover_piano_roll[:cover_piano_roll.shape[-1]//2, :]\n",
    "\n",
    "        song_piano_roll = np.load(song_file_path)\n",
    "        cover_piano_roll = np.load(cover_file_path)\n",
    "        \n",
    "        song_length = song_piano_roll.shape[0]\n",
    "        cover_length = cover_piano_roll.shape[0]\n",
    "        \n",
    "        out_data.append((song_piano_roll, cover_piano_roll))\n",
    "\n",
    "\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_paths(model, lr, load_batch_size, training_data, validation_data, \n",
    "                      num_epochs, device, loss_func=\"mse\", load_ckpt=None):\n",
    "\n",
    "    # get current time\n",
    "    start = time.time()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    if load_ckpt is not None and CKPT_PATH is not None:\n",
    "        print(\"LOADING FROM CKPT:\", load_ckpt)\n",
    "        checkpoint = torch.load(os.path.join(CKPT_PATH, load_ckpt))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "\n",
    "    batch_size = 1\n",
    "\n",
    "    # postprocess is broken\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_total = 0.0\n",
    "        val_loss_total = 0.0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        data_path_batched = []\n",
    "\n",
    "        for i, data_path in enumerate(training_data):\n",
    "\n",
    "            data_path_batched.append(data_path)\n",
    "\n",
    "            if ((i + 1) % load_batch_size == 0) or i == len(training_data) - 1: # only load when we accumulated enough\n",
    "                prep_load_batch_size = load_batch_size if (i + 1) % load_batch_size == 0 else len(training_data) % load_batch_size\n",
    "                batch_data = prepare_batch_sample(data_path_batched, prep_load_batch_size, \"train\")\n",
    "                train_loader = DataLoader(batch_data, batch_size=batch_size, shuffle=True) # this still has to be 1 due to vram constraints\n",
    "\n",
    "                for data in train_loader:\n",
    "                    songs = data[0].to(device)\n",
    "\n",
    "                    covers = data[1].to(device)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    outputs = model(songs)\n",
    "                    outputs = outputs.to(device)\n",
    "                    outputs = postprocess(outputs, covers)\n",
    "\n",
    "                    # pad tensors to same length\n",
    "                    if outputs.shape[1] > covers.shape[1]:\n",
    "                        covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                    elif covers.shape[1] > outputs.shape[1]:\n",
    "                        outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                    assert(outputs.shape == covers.shape)\n",
    "                    \n",
    "                    if loss_func == \"custom\":\n",
    "                        loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                    elif loss_func == \"mse\":\n",
    "                        loss = criterion(outputs, covers)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "\n",
    "                    loss.backward(retain_graph = True)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss_total += loss.item()\n",
    "                    \n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    # Add any other information you want to save (e.g., training loss, validation loss, etc.)\n",
    "                }\n",
    "\n",
    "                torch.save(checkpoint, f'/media/allentao/One Touch/APS360/ckpts/checkpoint_epoch{epoch + 1}.pt')\n",
    "\n",
    "                # reset\n",
    "                data_path_batched = []\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        data_path_batched = []\n",
    "        with torch.no_grad():\n",
    "            for i, data_path in enumerate(validation_data):\n",
    "                data_path_batched.append(data_path)\n",
    "\n",
    "                if ((i + 1) % load_batch_size == 0) or i == len(validation_data) - 1: # only load when we accumulated enough\n",
    "                    prep_load_batch_size = load_batch_size if (i + 1) % load_batch_size == 0 else len(validation_data) % load_batch_size\n",
    "                    batch_data = prepare_batch_sample(data_path_batched, prep_load_batch_size, \"val\")\n",
    "                    validation_loader = DataLoader(batch_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "                    for data in validation_loader:\n",
    "\n",
    "                        songs = data[0].to(device)\n",
    "\n",
    "                        covers = data[1].to(device)\n",
    "\n",
    "                        outputs = model(songs)\n",
    "                        outputs = outputs.to(device)\n",
    "                        outputs = postprocess(outputs, covers)\n",
    "\n",
    "                        # pad tensors to same length\n",
    "                        if outputs.shape[1] > covers.shape[1]:\n",
    "                            covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                        elif covers.shape[1] > outputs.shape[1]:\n",
    "                            outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                        assert(outputs.shape == covers.shape)\n",
    "                        \n",
    "                        if loss_func == \"custom\":\n",
    "                            loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                        elif loss_func == \"mse\":\n",
    "                            loss = criterion(outputs, covers)\n",
    "                        else:\n",
    "                            raise NotImplementedError\n",
    "                        \n",
    "                        val_loss_total += loss.item()\n",
    "                        \n",
    "                    data_path_batched = []\n",
    "                    \n",
    "        train_loss[epoch] = train_loss_total\n",
    "        val_loss[epoch] = val_loss_total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                f'Train Loss: {train_loss_total:.7f}, Train Loss: {train_loss_total:.7f}, '\n",
    "                f'Val Loss: {val_loss_total:.7f}, Val Loss: {val_loss_total:.7f}')\n",
    "        print(\"Time elapsed:\", time.time() - start)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    model_path = str(lr) + '_' + str(batch_size) + '_' + str(num_epochs)\n",
    "    torch.save(model.state_dict(), 'model' + model_path)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_loss)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_paths_test(model, load_batch_size, test_data, \n",
    "                      device, loss_func=\"mse\", load_ckpt=None):\n",
    "\n",
    "    # get current time\n",
    "    start = time.time()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    if load_ckpt is not None and CKPT_PATH is not None:\n",
    "        print(\"LOADING FROM CKPT:\", load_ckpt)\n",
    "        checkpoint = torch.load(os.path.join(CKPT_PATH, load_ckpt))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    batch_size = 1\n",
    "\n",
    "    test_loss_total = 0.0\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    data_path_batched = []\n",
    "    with torch.no_grad():\n",
    "        for i, data_path in enumerate(test_data):\n",
    "            data_path_batched.append(data_path)\n",
    "\n",
    "            if ((i + 1) % load_batch_size == 0) or i == len(test_data) - 1: # only load when we accumulated enough\n",
    "                prep_load_batch_size = load_batch_size if (i + 1) % load_batch_size == 0 else len(test_data) % load_batch_size\n",
    "                batch_data = prepare_batch_sample(data_path_batched, prep_load_batch_size, \"val\")\n",
    "                validation_loader = DataLoader(batch_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "                for data in validation_loader:\n",
    "\n",
    "                    songs = data[0].to(device)\n",
    "\n",
    "                    covers = data[1].to(device)\n",
    "\n",
    "                    outputs = model(songs)\n",
    "                    outputs = outputs.to(device)\n",
    "                    outputs = postprocess(outputs, covers)\n",
    "\n",
    "                    # pad tensors to same length\n",
    "                    if outputs.shape[1] > covers.shape[1]:\n",
    "                        covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                    elif covers.shape[1] > outputs.shape[1]:\n",
    "                        outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                    assert(outputs.shape == covers.shape)\n",
    "                    \n",
    "                    if loss_func == \"custom\":\n",
    "                        loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                    elif loss_func == \"mse\":\n",
    "                        loss = criterion(outputs, covers)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                    \n",
    "                    test_loss_total += loss.item()\n",
    "                    \n",
    "                data_path_batched = []\n",
    "\n",
    "                print(\"Test loss:\", test_loss_total)\n",
    "                print(\"Time elapsed:\", time.time() - start)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\" # force cpu\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(width = 3, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ../data/clean_data/0_0_song.midi ../data/clean_data/0_0_cover.midi\n",
      "Processed 0 songs\n",
      "Parsing ../data/clean_data/0_1_song.midi ../data/clean_data/0_1_cover.midi\n",
      "Processed 1 songs\n",
      "Parsing ../data/clean_data/0_2_song.midi ../data/clean_data/0_2_cover.midi\n",
      "Processed 2 songs\n",
      "Parsing ../data/clean_data/1_0_song.midi ../data/clean_data/1_0_cover.midi\n",
      "Processed 3 songs\n",
      "Parsing ../data/clean_data/1_1_song.midi ../data/clean_data/1_1_cover.midi\n",
      "Processed 4 songs\n",
      "Parsing ../data/clean_data/1_2_song.midi ../data/clean_data/1_2_cover.midi\n",
      "Processed 5 songs\n",
      "Parsing ../data/clean_data/2_0_song.midi ../data/clean_data/2_0_cover.midi\n",
      "Processed 6 songs\n",
      "Parsing ../data/clean_data/2_1_song.midi ../data/clean_data/2_1_cover.midi\n",
      "Processed 7 songs\n",
      "Parsing ../data/clean_data/2_2_song.midi ../data/clean_data/2_2_cover.midi\n",
      "Processed 8 songs\n",
      "Parsing ../data/clean_data/3_0_song.midi ../data/clean_data/3_0_cover.midi\n",
      "Parsing ../data/clean_data/3_1_song.midi ../data/clean_data/3_1_cover.midi\n",
      "Processed 9 songs\n",
      "Parsing ../data/clean_data/3_2_song.midi ../data/clean_data/3_2_cover.midi\n",
      "Processed 10 songs\n",
      "Parsing ../data/clean_data/3_3_song.midi ../data/clean_data/3_3_cover.midi\n",
      "Processed 11 songs\n",
      "Parsing ../data/clean_data/4_0_song.midi ../data/clean_data/4_0_cover.midi\n",
      "Processed 12 songs\n",
      "Parsing ../data/clean_data/4_1_song.midi ../data/clean_data/4_1_cover.midi\n",
      "Processed 13 songs\n",
      "Parsing ../data/clean_data/4_2_song.midi ../data/clean_data/4_2_cover.midi\n",
      "Processed 14 songs\n",
      "Parsing ../data/clean_data/5_0_song.midi ../data/clean_data/5_0_cover.midi\n",
      "Parsing ../data/clean_data/5_1_song.midi ../data/clean_data/5_1_cover.midi\n",
      "Processed 15 songs\n",
      "Parsing ../data/clean_data/6_0_song.midi ../data/clean_data/6_0_cover.midi\n",
      "Processed 16 songs\n",
      "Parsing ../data/clean_data/6_1_song.midi ../data/clean_data/6_1_cover.midi\n",
      "Processed 17 songs\n",
      "Parsing ../data/clean_data/6_2_song.midi ../data/clean_data/6_2_cover.midi\n",
      "Processed 18 songs\n",
      "Parsing ../data/clean_data/6_3_song.midi ../data/clean_data/6_3_cover.midi\n",
      "Processed 19 songs\n",
      "Parsing ../data/clean_data/6_4_song.midi ../data/clean_data/6_4_cover.midi\n",
      "Processed 20 songs\n"
     ]
    }
   ],
   "source": [
    "# WARNING: on single you only need to run this ONCE!\n",
    "if LOADER == \"batch\":\n",
    "    training_data, validation_data, testing_data = get_data()\n",
    "elif LOADER == \"single\":\n",
    "    training_data, validation_data, testing_data = get_data_paths(ALREADY_LOADED)\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOADER == \"batch\" and SETTING == \"train\":\n",
    "    model_train(model, 0.1, BATCH_SIZE, training_data, validation_data, 200, device, loss_func=\"mse\")\n",
    "elif LOADER == \"single\" and SETTING == \"train\":\n",
    "    model_train_paths(model, 0.0001, LOAD_BATCH_SIZE, training_data, validation_data, 200, \\\n",
    "                      device, loss_func=\"mse\", load_ckpt=\"checkpoint_epoch1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING FROM CKPT: checkpoint_epoch121.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 7.786501407623291\n",
      "Time elapsed: 10.042664766311646\n"
     ]
    }
   ],
   "source": [
    "# Get testing loss using blur loss\n",
    "model_train_paths_test(model, LOAD_BATCH_SIZE, testing_data, device, loss_func=\"custom\", load_ckpt=\"checkpoint_epoch121.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy function\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "\n",
    "def midi_note_pitch_accuracy(piano_roll1, piano_roll2, start_time1=None, end_time1=None, start_time2=None, end_time2=None):\n",
    "\n",
    "    # If start and end times are specified, convert them to frame indices and slice the piano rolls\n",
    "    if start_time1 is not None and end_time1 is not None:\n",
    "        start_frame1 = int(start_time1 * 100)\n",
    "        end_frame1 = int(end_time1 * 100)\n",
    "        piano_roll1 = piano_roll1[:, start_frame1:end_frame1]\n",
    "\n",
    "    if start_time2 is not None and end_time2 is not None:\n",
    "        start_frame2 = int(start_time2 * 100)\n",
    "        end_frame2 = int(end_time2 * 100)\n",
    "        piano_roll2 = piano_roll2[:, start_frame2:end_frame2]\n",
    "\n",
    "    # If the piano rolls have different numbers of columns, truncate the longer one to match the shorter one\n",
    "    min_length = min(piano_roll1.shape[1], piano_roll2.shape[1])\n",
    "    piano_roll1 = piano_roll1[:, :min_length]\n",
    "    piano_roll2 = piano_roll2[:, :min_length]\n",
    "\n",
    "    # Get the note pitch at each time step by finding the index of the maximum value in each column\n",
    "    note_sequence1 = np.argmax(piano_roll1, axis=0)\n",
    "    note_sequence2 = np.argmax(piano_roll2, axis=0)\n",
    "\n",
    "    # Compute the note pitch accuracy by comparing the note sequences\n",
    "    correct_notes = np.sum(note_sequence1 == note_sequence2)\n",
    "    total_notes = len(note_sequence1)\n",
    "    note_accuracy = correct_notes / total_notes\n",
    "\n",
    "    return note_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate: loads the model checkpoints and computes the losses and accuracies\n",
    "def evaluate(trained_epoch, load_batch_size, training_data, validation_data, \n",
    "              device, loss_func=\"mse\"):\n",
    "\n",
    "    # Load the current checkpoint\n",
    "    cur_ckpt = os.path.join(CKPT_PATH, \"checkpoint_epoch\" + str(trained_epoch) + \".pt\")\n",
    "    state = torch.load(cur_ckpt)\n",
    "    model = Net(width = 3, batch_size = 1)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "\n",
    "    # Compute losses and accuracies\n",
    "    train_loss_total = 0.0\n",
    "    train_acc_total = 0.0\n",
    "    val_loss_total = 0.0\n",
    "    val_acc_total = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    batch_size = 1\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    data_path_batched = []\n",
    "\n",
    "    for i, data_path in enumerate(training_data):\n",
    "\n",
    "        data_path_batched.append(data_path)\n",
    "\n",
    "        if ((i + 1) % load_batch_size == 0) or i == len(training_data) - 1: # only load when we accumulated enough\n",
    "            prep_load_batch_size = load_batch_size if (i + 1) % load_batch_size == 0 else len(training_data) % load_batch_size\n",
    "            batch_data = prepare_batch_sample(data_path_batched, prep_load_batch_size, \"train\")\n",
    "            train_loader = DataLoader(batch_data, batch_size=batch_size, shuffle=True) # this still has to be 1 due to vram constraints\n",
    "\n",
    "            for data in train_loader:\n",
    "                songs = data[0].to(device)\n",
    "\n",
    "                covers = data[1].to(device)\n",
    "\n",
    "                outputs = model(songs)\n",
    "                outputs = outputs.to(device)\n",
    "                outputs = postprocess(outputs, covers)\n",
    "\n",
    "                # pad tensors to same length\n",
    "                if outputs.shape[1] > covers.shape[1]:\n",
    "                    covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                elif covers.shape[1] > outputs.shape[1]:\n",
    "                    outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                assert(outputs.shape == covers.shape)\n",
    "                \n",
    "                if loss_func == \"custom\":\n",
    "                    loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                elif loss_func == \"mse\":\n",
    "                    loss = criterion(outputs, covers)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "                train_loss_total += loss.item().detach().numpy()\n",
    "\n",
    "                train_acc_total += midi_note_pitch_accuracy(outputs.to(\"cpu\"), covers.to(\"cpu\"))\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # reset\n",
    "            data_path_batched = []\n",
    "\n",
    "    # Validation\n",
    "    data_path_batched = []\n",
    "    with torch.no_grad():\n",
    "        for i, data_path in enumerate(validation_data):\n",
    "            data_path_batched.append(data_path)\n",
    "\n",
    "            if ((i + 1) % load_batch_size == 0) or i == len(validation_data) - 1: # only load when we accumulated enough\n",
    "                prep_load_batch_size = load_batch_size if (i + 1) % load_batch_size == 0 else len(validation_data) % load_batch_size\n",
    "                batch_data = prepare_batch_sample(data_path_batched, prep_load_batch_size, \"val\")\n",
    "                validation_loader = DataLoader(batch_data, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "                for data in validation_loader:\n",
    "\n",
    "                    songs = data[0].to(device)\n",
    "\n",
    "                    covers = data[1].to(device)\n",
    "\n",
    "                    outputs = model(songs)\n",
    "                    outputs = outputs.to(device)\n",
    "                    outputs = postprocess(outputs, covers)\n",
    "\n",
    "                    # pad tensors to same length\n",
    "                    if outputs.shape[1] > covers.shape[1]:\n",
    "                        covers = F.pad(covers, (0, 0, 0, outputs.shape[1] - covers.shape[1]))\n",
    "                    elif covers.shape[1] > outputs.shape[1]:\n",
    "                        outputs = F.pad(outputs, (0, 0, 0, covers.shape[1] - outputs.shape[1]))\n",
    "                    assert(outputs.shape == covers.shape)\n",
    "                    \n",
    "                    if loss_func == \"custom\":\n",
    "                        loss = blur_loss(outputs, covers, device) + criterion(outputs, covers) # warning: memory intensive\n",
    "                    elif loss_func == \"mse\":\n",
    "                        loss = criterion(outputs, covers)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                    \n",
    "                    val_loss_total += loss.item().detach().numpy()\n",
    "\n",
    "                    val_acc_total += midi_note_pitch_accuracy(outputs.to(\"cpu\"), covers.to(\"cpu\"))\n",
    "                    \n",
    "                data_path_batched = []\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return train_loss_total, train_acc_total, val_loss_total, val_acc_total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model from each epoch and plot its training and validation loss and accuracies\n",
    "# TRAINED_EPOCHS = 121\n",
    "\n",
    "# # Arrays for plotting\n",
    "# train_losses = []\n",
    "# train_accs = []\n",
    "\n",
    "# val_losses = []\n",
    "# val_accs = []\n",
    "\n",
    "# for epoch in range(1, TRAINED_EPOCHS + 1):\n",
    "#     # ckpt_path = \"/media/allentao/One Touch/APS360/ckpts/aug1lr0.0001overfit/checkpoint_epoch121.pt\"\n",
    "#     train_loss, train_acc, val_loss, val_acc = evaluate(epoch, LOAD_BATCH_SIZE, training_data, validation_data, device, loss_func=\"mse\")\n",
    "    \n",
    "#     train_losses.append(train_loss)\n",
    "#     train_accs.append(train_acc)\n",
    "#     val_losses.append(val_loss)\n",
    "#     val_accs.append(val_acc)\n",
    "\n",
    "#     print(\"Epoch: {} | Train Loss: {} | Train Acc: {} | Val Loss: {} | Val Acc: {}\".format(epoch, train_loss, train_acc, val_loss, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the plots (quantitative evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: generate confusion matrix or etc\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aps360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
